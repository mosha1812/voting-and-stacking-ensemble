{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae6d4a7",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/ensemble-learning-stacking-blending-voting-b37737c4f483\n",
    "\n",
    "There are different types of Ensemble Learning techniques which differ mainly by:\n",
    "\n",
    "    the type of models used (homogeneous or heterogeneous models), \n",
    "    the data sampling (with or without replacement, k-fold, etc.) and \n",
    "    the decision function (voting, average, meta model, etc).\n",
    "    \n",
    "Therefore, Ensemble Learning techniques can be classified as:\n",
    "    \n",
    "    Bagging\n",
    "    Boosting\n",
    "    Stacking\n",
    "\n",
    "\n",
    "In addition to these three main categories, two important variations emerge: \n",
    "    \n",
    "    Voting (which is a complement of Bagging) and \n",
    "    Blending (a subtype of Stacking). \n",
    "\n",
    "Although Voting and Blending are a complement and a subtype of Bagging and Stacking respectively, these techniques are often found as direct types of Ensemble Learning.\n",
    "\n",
    "\n",
    "**Stacking:**\n",
    "\n",
    "Better known as Stacking Generalization, the key is to reduce the generalization error of different generalizers (i.e. ML models). \n",
    "\n",
    "The general idea of the Stacking Generalization method is the generation of a Meta-Model. \n",
    "\n",
    "The Stacking Generalization method is commonly composed of 2 training stages, “level 0” and “level 1”. \n",
    "\n",
    "It is important to mention that it can be added as many levels as necessary. However, in practice it is common to use only 2 levels. \n",
    "\n",
    "The aim of the first stage (level 0) is to generate the training data for the meta-model, this is carried out by implementing k-fold cross validation for each “weak learner” defined in the first stage. \n",
    "\n",
    "The predictions of each one of these“weak learners” are “stacked” in order to build such such “new training set” (the meta-model). \n",
    "\n",
    "The aim of the second stage (level 1) is to train the meta-model, such training is carried out through an already determined “final learner”.\n",
    "\n",
    "\n",
    "**Blending:**\n",
    "\n",
    "Blending is a technique derived from Stacking Generalization. \n",
    "\n",
    "The only difference is that in Blending, the k-fold cross validation technique is not used to generate the training data of the meta-model. \n",
    "\n",
    "Blending implements “one-holdout set”, that is, a small portion of the training data (validation) to make predictions which will be “stacked” to form the training data of the meta-model. \n",
    "\n",
    "Also, predictions are made from the test data to form the meta-model test data.\n",
    "\n",
    "\n",
    "**Voting:**\n",
    "\n",
    "The Voting Classifier is a homogeneous and heterogeneous type of Ensemble Learning, that is, the base classifiers can be of the same or different type. \n",
    "\n",
    "This type of ensemble also works as an extension of bagging (e.g. Random Forest).\n",
    "\n",
    "The architecture of a Voting Classifier is made up of a number “n” of ML models, whose predictions are valued in two different ways: \n",
    "\n",
    "    Hard:\n",
    "        In ***hard*** mode, the winning prediction is the one with “the most votes”. \n",
    "        \n",
    "    Soft:\n",
    "        ***Soft*** mode considers the probabilities thrown by each ML model, \n",
    "        these probabilities will be weighted and averaged, \n",
    "        consequently the winning class will be the one with the highest weighted and averaged probability.\n",
    "\n",
    "\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae1ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "#from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a262a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870e822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = pd.read_csv('data/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1710ca8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf023671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "diabetes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506d2a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 False\n",
       "Glucose                     False\n",
       "BloodPressure               False\n",
       "SkinThickness               False\n",
       "Insulin                     False\n",
       "BMI                         False\n",
       "DiabetesPedigreeFunction    False\n",
       "Age                         False\n",
       "Outcome                     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4c54b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624f4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes_df.drop(columns=['Outcome']), \n",
    "                                                    diabetes_df['Outcome'], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8d1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GaussianNB()\n",
    "clf2 = LogisticRegression(random_state=32)\n",
    "clf3 = DecisionTreeClassifier(random_state=32)\n",
    "clf4 = RandomForestClassifier(random_state=32)\n",
    "clf5 = KNeighborsClassifier(n_neighbors=1)\n",
    "# For SVC probability parameter needed to be set to True to use with soft voting classifier\n",
    "clf6 = SVC(probability=True, random_state=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cc697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [clf1, clf2, clf3, clf4, clf5, clf6]\n",
    "labels = ['Naive Bayes', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'K-Nearest Neighbors', 'SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6babb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3dc7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 (+/- 0.05) [Naive Bayes]\n",
      "Accuracy: 0.77 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.73 (+/- 0.06) [Decision Tree]\n",
      "Accuracy: 0.76 (+/- 0.06) [Random Forest]\n",
      "Accuracy: 0.69 (+/- 0.06) [K-Nearest Neighbors]\n",
      "Accuracy: 0.74 (+/- 0.05) [SVC]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip(models, labels):\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c54cb2",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1487f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_hard = VotingClassifier(estimators = list(zip(labels,models)), voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3441e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.05) [voting_clf_hard]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(voting_clf_hard, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'voting_clf_hard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b699ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7835497835497836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.84       147\n",
      "           1       0.80      0.54      0.64        84\n",
      "\n",
      "    accuracy                           0.78       231\n",
      "   macro avg       0.79      0.73      0.74       231\n",
      "weighted avg       0.79      0.78      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_clf_hard.fit(X_train, y_train)\n",
    "y_pred = voting_clf_hard.predict(X_test)\n",
    "\n",
    "# evaluating model performance using accuracy_score and classification report\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "clf_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8dee82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_soft = VotingClassifier(estimators = list(zip(labels,models)), voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b3dfdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.05) [voting_clf_soft]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(voting_clf_soft, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'voting_clf_soft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3181f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575757575757576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       147\n",
      "           1       0.71      0.57      0.63        84\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.72      0.73       231\n",
      "weighted avg       0.75      0.76      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_clf_soft.fit(X_train, y_train)\n",
    "y_pred = voting_clf_soft.predict(X_test)\n",
    "\n",
    "# evaluating model performance using accuracy_score and classification report\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "clf_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992a3c4",
   "metadata": {},
   "source": [
    "# Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1535b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GaussianNB()\n",
    "clf2 = LogisticRegression(random_state=32)\n",
    "clf3 = DecisionTreeClassifier(random_state=32)\n",
    "clf4 = RandomForestClassifier(random_state=32)\n",
    "clf5 = KNeighborsClassifier(n_neighbors=1)\n",
    "# For SVC probability parameter needed to be set to True to use with soft voting classifier\n",
    "clf6 = SVC(probability=True, random_state=32) \n",
    "\n",
    "meta_clf = LogisticRegression(random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd516533",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [clf1, clf2, clf3, clf4, clf5, clf6]\n",
    "labels = ['Naive Bayes', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'K-Nearest Neighbors', 'SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec0214be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_clf = StackingClassifier(list(zip(labels,models)), meta_clf)\n",
    "\n",
    "models.append(stack_clf)\n",
    "labels.append('Stacking Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a54ba48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 (+/- 0.05) [Naive Bayes]\n",
      "Accuracy: 0.77 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.73 (+/- 0.06) [Decision Tree]\n",
      "Accuracy: 0.76 (+/- 0.06) [Random Forest]\n",
      "Accuracy: 0.69 (+/- 0.06) [K-Nearest Neighbors]\n",
      "Accuracy: 0.74 (+/- 0.05) [SVC]\n",
      "Accuracy: 0.77 (+/- 0.06) [Stacking Classifier]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip(models, labels):\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "022a333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7662337662337663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       147\n",
      "           1       0.72      0.58      0.64        84\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.75      0.73      0.74       231\n",
      "weighted avg       0.76      0.77      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack_clf.fit(X_train, y_train)\n",
    "y_pred = stack_clf.predict(X_test)\n",
    "\n",
    "# evaluating model performance using accuracy_score and classification report\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "clf_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588516bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
